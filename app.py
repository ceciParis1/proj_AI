import streamlit as st
import os
import dotenv
import uuid
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage
from rag_methods import (
    get_poems_from_poetrydb,
    stream_llm_response,
    stream_llm_rag_response,
)

dotenv.load_dotenv()

# Configuration de la page
st.set_page_config(
    page_title="Recherche et Analyse de Po√®mes avec LLM", 
    page_icon="üìö", 
    layout="centered", 
    initial_sidebar_state="expanded"
)

# Ins√©rer directement la cl√© API ici (c'est temporaire et non recommand√© pour la production)
api_key = "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"

# V√©rifier si la cl√© est bien pr√©sente
if not api_key:
    st.error("Cl√© API OpenAI manquante. Assurez-vous de l'ins√©rer correctement dans le fichier app.py.")
else:
    st.success("Cl√© API OpenAI charg√©e avec succ√®s.")

# Initialiser le mod√®le OpenAI
llm_stream = ChatOpenAI(
    api_key=api_key,  # Utiliser la cl√© API ici
    model_name="gpt-4",
    temperature=0.7,
    streaming=True,
)

# Initialisation de la session
if "session_id" not in st.session_state:
    st.session_state.session_id = str(uuid.uuid4())

if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "assistant", "content": "Bienvenue ! Je peux vous aider √† trouver des po√®mes et √† en discuter ou les analyser."}
    ]

# Affichage des messages de la session
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Saisie d'un nouveau message par l'utilisateur
if prompt := st.chat_input("Votre message"):
    # V√©rification si le message contient un th√®me pour la recherche de po√®mes
    if "th√®me" in prompt.lower():
        # Extraire le th√®me du message
        theme = prompt.split("th√®me")[-1].strip()
        linecount = st.number_input("Longueur des vers (en nombre de lignes, optionnel)", min_value=1, step=1)
        st.session_state.messages.append({"role": "user", "content": f"Recherche de po√®mes sur le th√®me '{theme}' avec {linecount} vers."})

        # Recherche de po√®mes via PoetryDB
        poems = get_poems_from_poetrydb(theme=theme, linecount=linecount)

        if poems:
            for poem in poems:
                with st.chat_message("assistant"):
                    st.markdown(f"**{poem['title']}**\n\n" + "\n".join(poem['lines']))
                    st.session_state.messages.append({"role": "assistant", "content": f"**{poem['title']}**\n\n" + "\n".join(poem['lines'])})

            # Utilisation du LLM pour fournir une analyse litt√©raire
            with st.chat_message("assistant"):
                user_prompt = f"Voici un po√®me sur {theme} avec {linecount} vers. Peux-tu en faire une analyse litt√©raire ?"
                messages = [HumanMessage(content=user_prompt)]

                full_response = ""
                placeholder = st.empty()  # Cr√©er un espace r√©serv√© pour accumuler et afficher les r√©sultats

                for chunk in llm_stream.stream(messages):
                    full_response += chunk.content  # Ajout du texte sans espaces suppl√©mentaires
                    # Ajout d'une mise en forme du po√®me avec deux sauts de ligne pour s√©parer les strophes
                    formatted_text = full_response.replace('\n', '  \n')  
                    placeholder.markdown(formatted_text)  # Affichage du texte accumul√© progressivement avec la bonne mise en forme

                st.session_state.messages.append({"role": "assistant", "content": full_response})
        else:
            st.warning("Aucun po√®me trouv√© avec les crit√®res sp√©cifi√©s.")
            st.session_state.messages.append({"role": "assistant", "content": "Aucun po√®me trouv√© avec les crit√®res sp√©cifi√©s."})
    else:
        # Enregistrement du message utilisateur
        st.session_state.messages.append({"role": "user", "content": prompt})

        with st.chat_message("user"):
            st.markdown(prompt)

        # R√©ponse du LLM
        with st.chat_message("assistant"):
            # V√©rification que le prompt n'est pas vide
            if prompt:
                messages = [HumanMessage(content=prompt)]
                full_response = ""
                placeholder = st.empty()  # Espace r√©serv√© pour la r√©ponse

                # It√©ration sur le g√©n√©rateur pour afficher la r√©ponse du LLM
                for chunk in stream_llm_response(llm_stream, messages):
                    full_response += chunk  # Ajout du chunk sans espace suppl√©mentaire
                    # Ajout de la mise en forme avec des retours √† la ligne appropri√©s
                    formatted_text = full_response.replace('\n', '  \n')
                    placeholder.markdown(formatted_text)  # Affichage du chunk dans l'interface

                # Enregistrement de la r√©ponse compl√®te dans l'√©tat de session
                st.session_state.messages.append({"role": "assistant", "content": full_response})
            else:
                st.warning("Veuillez saisir un message avant de soumettre.")
