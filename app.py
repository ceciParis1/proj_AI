import streamlit as st
import os
import dotenv
import uuid
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage
from rag_methods import (
    get_poems_from_poetrydb,
    stream_llm_response,
    stream_llm_rag_response,
)

dotenv.load_dotenv()

# R√©cup√©rer la cl√© API OpenAI
api_key = os.getenv("OPENAI_API_KEY")

if not api_key:
    raise ValueError("Cl√© API OpenAI manquante. Assurez-vous qu'elle est d√©finie dans les secrets Streamlit Cloud.")

# Initialiser le mod√®le OpenAI
llm_stream = ChatOpenAI(
    api_key=api_key,  # Utilisation de la cl√© API
    model_name="gpt-4",  # Mod√®le GPT-4
    temperature=0.7,
    streaming=True,
)

# Configuration de la page
st.set_page_config(
    page_title="Recherche et Analyse de Po√®mes avec LLM", 
    page_icon="üìö", 
    layout="centered", 
    initial_sidebar_state="expanded"
)

# Initialisation
if "session_id" not in st.session_state:
    st.session_state.session_id = str(uuid.uuid4())

if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "assistant", "content": "Bienvenue ! Je peux vous aider √† trouver des po√®mes et √† en discuter ou les analyser."}
    ]

# Affichage des messages de la session
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])


# Sidebar pour la recherche de po√®mes
with st.sidebar:
    st.header("Rechercher des po√®mes")
    theme = st.text_input("Th√®me du po√®me (optionnel)")
    linecount = st.number_input("Longueur des vers (en nombre de lignes, optionnel)", min_value=1, step=1)
    search_button = st.button("Rechercher des po√®mes")

# Quand l'utilisateur clique sur "Rechercher"
if search_button:
    st.session_state.messages.append({"role": "user", "content": f"Recherche de po√®mes sur le th√®me '{theme}' avec {linecount} vers."})

    # Recherche de po√®mes via PoetryDB
    poems = get_poems_from_poetrydb(theme=theme, linecount=linecount)

    if poems:
        for poem in poems:
            with st.chat_message("assistant"):
                st.markdown(f"**{poem['title']}**\n\n" + "\n".join(poem['lines']))
                st.session_state.messages.append({"role": "assistant", "content": f"**{poem['title']}**\n\n" + "\n".join(poem['lines'])})

        # Utilisation du LLM pour fournir une analyse litt√©raire
        with st.chat_message("assistant"):
            user_prompt = f"Voici un po√®me sur {theme} avec {linecount} vers. Peux-tu en faire une analyse litt√©raire ?"
            messages = [HumanMessage(content=user_prompt)]
            
            full_response = ""
            for chunk in llm_stream.stream(messages):
                full_response += chunk.content
                st.markdown(chunk.content)

            st.session_state.messages.append({"role": "assistant", "content": full_response})
    else:
        st.warning("Aucun po√®me trouv√© avec les crit√®res sp√©cifi√©s.")
        st.session_state.messages.append({"role": "assistant", "content": "Aucun po√®me trouv√© avec les crit√®res sp√©cifi√©s."})

# Saisie d'un nouveau message par l'utilisateur
if prompt := st.chat_input("Votre message"):
    st.session_state.messages.append({"role": "user", "content": prompt})

    with st.chat_message("user"):
        st.markdown(prompt)

    # R√©ponse du LLM
    with st.chat_message("assistant"):
        messages = [HumanMessage(content=prompt)]
        response = stream_llm_response(llm_stream, messages)
        st.markdown(response)
